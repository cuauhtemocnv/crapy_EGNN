{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d82d3b-8a46-4eb9-a951-c10261d3a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1-2): 2 x Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.models.egnn import *\n",
    "from src.models.utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f4267f-e15f-42af-8a65-7e146fa11b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.1044],\n",
       "          [-0.1213],\n",
       "          [-0.1161],\n",
       "          [-0.1174],\n",
       "          [-0.1079],\n",
       "          [-0.1029]], grad_fn=<MeanBackward1>),\n",
       "  tensor(0.0161, grad_fn=<MeanBackward0>)),\n",
       " (tensor([[0.5264, 0.1540],\n",
       "          [0.9918, 0.3277],\n",
       "          [0.7347, 0.0501],\n",
       "          [0.2242, 0.9223],\n",
       "          [0.7990, 0.3257],\n",
       "          [0.4714, 0.5287]], grad_fn=<MeanBackward1>),\n",
       "  tensor(0.5047, grad_fn=<MeanBackward0>)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#create a model\n",
    "# Hyperparameters\n",
    "num_nodes = 6\n",
    "feature_dim = 1\n",
    "hidden_dim =10\n",
    "output_dim = 1\n",
    "cutoff = 1.5\n",
    "device = 'cpu'\n",
    "\n",
    "# Initialize model\n",
    "model = EGNN(\n",
    "    in_node_nf=feature_dim,\n",
    "    hidden_nf=hidden_dim,\n",
    "    out_node_nf=output_dim,\n",
    "    n_layers=3\n",
    ").to(device)\n",
    "\n",
    "model.train()  # Keep dropout active\n",
    "\n",
    "node_features, node_coords, edge_index = generate_data(num_nodes, feature_dim, cutoff)\n",
    "target_coords = update_coordinates(node_coords,node_features)\n",
    "target_features = torch.norm(node_features)\n",
    "model.forward_with_uncertainty(node_features, node_coords, edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6128b8-da00-47ac-acaa-e3dc694f1a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.04930133745074272 0\n",
      "loss 0.03286547213792801 1\n",
      "loss 0.0327131412923336 2\n",
      "loss 0.01496561337262392 3\n",
      "loss 0.018054941669106483 4\n",
      "loss 0.015099638141691685 5\n",
      "loss 0.020451869815587997 6\n",
      "loss 0.013500969856977463 7\n",
      "Got example 0.028154252097010612 looshx 1.244702935218811 0.028154252097010612 uncertx tensor(0.7102, grad_fn=<MeanBackward0>) 0 8\n",
      "loss 0.025674059987068176 8\n",
      "loss 0.014373533427715302 9\n",
      "loss 0.029692530632019043 10\n",
      "loss 0.034306593239307404 11\n",
      "loss 0.018139880150556564 12\n",
      "loss 0.01150169875472784 13\n",
      "loss 0.027780987322330475 14\n",
      "loss 0.019819026812911034 15\n",
      "loss 0.009010657668113708 16\n",
      "loss 0.01625998318195343 17\n",
      "loss 0.003098102519288659 18\n",
      "Got example 0.029427727684378624 looshx 1.2644520998001099 0.029427727684378624 uncertx tensor(0.7012, grad_fn=<MeanBackward0>) 1 19\n",
      "loss 0.005794097203761339 19\n",
      "Got example 0.015460394322872162 looshx 1.407173752784729 0.015460394322872162 uncertx tensor(0.7210, grad_fn=<MeanBackward0>) 2 20\n",
      "loss 0.019100842997431755 20\n",
      "loss 0.011591285467147827 21\n",
      "loss 0.013324111700057983 22\n",
      "loss 0.008287752978503704 23\n",
      "loss 0.013527746312320232 24\n",
      "Got example 0.0058671156875789165 looshx 0.47715920209884644 0.0058671156875789165 uncertx tensor(0.7035, grad_fn=<MeanBackward0>) 3 25\n",
      "loss 0.005755582824349403 25\n",
      "loss 0.019965501502156258 26\n",
      "loss 0.001089641940779984 27\n",
      "loss 0.012076609767973423 28\n",
      "loss 0.0033818529918789864 29\n",
      "loss 0.011611544527113438 30\n",
      "loss 0.010782267898321152 31\n",
      "loss 0.0027090031653642654 32\n",
      "loss 0.007950989529490471 33\n",
      "loss 0.01692338101565838 34\n",
      "loss 0.009846319444477558 35\n",
      "loss 0.010826961137354374 36\n",
      "loss 0.0075428239069879055 37\n",
      "loss 0.002102237893268466 38\n",
      "loss 0.0034401097800582647 39\n",
      "loss 0.0025526678655296564 40\n",
      "loss 0.0021534874103963375 41\n",
      "loss 0.006604216992855072 42\n",
      "loss 0.003843375714495778 43\n",
      "loss 0.009280262514948845 44\n",
      "loss 0.006128393113613129 45\n",
      "loss 0.0020019884686917067 46\n",
      "loss 0.007581584621220827 47\n",
      "loss 0.0018152035772800446 48\n",
      "loss 0.0022008027881383896 49\n",
      "loss 0.0011725731892511249 50\n",
      "loss 0.006552041042596102 51\n",
      "loss 0.012050699442625046 52\n",
      "loss 0.008494964800775051 53\n",
      "loss 0.002969688968732953 54\n",
      "loss 0.004729804117232561 55\n",
      "loss 0.006247682962566614 56\n",
      "loss 0.003094458021223545 57\n",
      "loss 0.004006833769381046 58\n",
      "loss 0.007688460871577263 59\n",
      "loss 0.0029034314211457968 60\n",
      "loss 0.010541453026235104 61\n",
      "loss 0.008005687035620213 62\n",
      "loss 0.005184742622077465 63\n",
      "loss 0.005665081087499857 64\n",
      "loss 0.0036976300179958344 65\n",
      "loss 0.0036950043868273497 66\n",
      "loss 0.002796417335048318 67\n",
      "loss 0.0034876354038715363 68\n",
      "loss 0.00240340456366539 69\n",
      "loss 0.0044721439480781555 70\n",
      "loss 0.0042060017585754395 71\n",
      "loss 0.00905472319573164 72\n",
      "loss 0.005757154896855354 73\n",
      "loss 0.004994748625904322 74\n",
      "loss 0.0011992583749815822 75\n",
      "loss 0.004544609691947699 76\n",
      "loss 0.014152937568724155 77\n",
      "loss 0.008352496661245823 78\n",
      "loss 0.005919641349464655 79\n",
      "loss 0.014473777264356613 80\n",
      "loss 0.0006361901760101318 81\n",
      "loss 0.0030774737242609262 82\n",
      "loss 0.0024954050313681364 83\n",
      "loss 0.00987077783793211 84\n",
      "loss 0.00381678924895823 85\n",
      "loss 0.00580441951751709 86\n",
      "loss 0.004203295335173607 87\n",
      "loss 0.0039842030964791775 88\n",
      "loss 0.005614489782601595 89\n",
      "loss 0.006291280034929514 90\n",
      "loss 0.016681253910064697 91\n",
      "loss 0.004340536426752806 92\n",
      "loss 0.005593609064817429 93\n",
      "loss 0.01130624394863844 94\n",
      "loss 0.002093012910336256 95\n",
      "loss 0.016491802409291267 96\n",
      "loss 0.00324300117790699 97\n",
      "loss 0.003966328222304583 98\n",
      "loss 0.0025673953350633383 99\n",
      "loss 0.010855345986783504 100\n",
      "loss 0.007579258177429438 101\n",
      "loss 0.006318786647170782 102\n",
      "loss 0.0014694146811962128 103\n",
      "loss 0.005422516260296106 104\n",
      "loss 0.00543889170512557 105\n",
      "loss 0.006663268897682428 106\n",
      "loss 0.0053927418775856495 107\n",
      "loss 0.009080451913177967 108\n",
      "loss 0.0017299657920375466 109\n",
      "loss 0.007338118273764849 110\n",
      "loss 0.006812286097556353 111\n",
      "loss 0.008645037189126015 112\n",
      "loss 0.0037333816289901733 113\n",
      "loss 0.0070265247486531734 114\n",
      "loss 0.006762866396456957 115\n",
      "loss 0.005508044268935919 116\n",
      "loss 0.006942963693290949 117\n",
      "loss 0.006535952445119619 118\n",
      "loss 0.006659403908997774 119\n",
      "loss 0.001294077024795115 120\n",
      "loss 0.003147524781525135 121\n",
      "loss 0.001862673438154161 122\n",
      "loss 0.006091224029660225 123\n",
      "loss 0.0033091839868575335 124\n",
      "loss 0.0025378598365932703 125\n",
      "loss 0.00533460034057498 126\n",
      "loss 0.008412553928792477 127\n",
      "loss 0.004473125096410513 128\n",
      "loss 0.01324121281504631 129\n",
      "loss 0.004661547485738993 130\n",
      "loss 0.006963823456317186 131\n",
      "loss 0.006597771774977446 132\n",
      "loss 0.0002653148549143225 133\n",
      "loss 0.003033093875274062 134\n",
      "loss 0.010151633061468601 135\n",
      "loss 0.0015133264241740108 136\n",
      "loss 0.004496271722018719 137\n",
      "loss 0.008325762115418911 138\n",
      "loss 0.013134527020156384 139\n",
      "loss 0.005852809175848961 140\n",
      "loss 0.0010787906358018517 141\n",
      "loss 0.015605911612510681 142\n",
      "loss 0.007342159748077393 143\n",
      "loss 0.0028145581018179655 144\n",
      "loss 0.006057373713701963 145\n",
      "loss 0.0024833937641233206 146\n",
      "loss 0.0036443474236875772 147\n",
      "loss 0.004777671303600073 148\n",
      "loss 0.0053202142007648945 149\n",
      "loss 0.0028868066146969795 150\n",
      "loss 0.0018956592539325356 151\n",
      "loss 0.006340887863188982 152\n",
      "loss 0.0034305837471038103 153\n",
      "loss 0.0034843136090785265 154\n",
      "loss 0.0018533878028392792 155\n",
      "loss 0.008996124379336834 156\n",
      "loss 0.0037769812624901533 157\n",
      "loss 0.00900458823889494 158\n",
      "loss 0.0020046636927872896 159\n",
      "loss 0.003119952976703644 160\n",
      "loss 0.0031281139235943556 161\n",
      "loss 0.002904339926317334 162\n",
      "loss 0.007804231718182564 163\n",
      "loss 0.006389405112713575 164\n",
      "loss 0.008472827263176441 165\n",
      "loss 0.008623888716101646 166\n",
      "loss 0.000644905143417418 167\n",
      "loss 0.004517969209700823 168\n",
      "loss 0.0033652379643172026 169\n",
      "loss 0.0045273685827851295 170\n",
      "loss 0.003812488168478012 171\n",
      "loss 0.0063179791904985905 172\n",
      "loss 0.012932452373206615 173\n",
      "loss 0.0022076701279729605 174\n",
      "loss 0.0124571667984128 175\n",
      "loss 0.0023628761991858482 176\n",
      "loss 0.008549913763999939 177\n",
      "loss 0.004525257274508476 178\n",
      "loss 0.008433893322944641 179\n",
      "loss 0.007140681613236666 180\n",
      "loss 0.003990620840340853 181\n",
      "loss 0.005006878171116114 182\n",
      "loss 0.005548762157559395 183\n",
      "loss 0.006426747888326645 184\n",
      "loss 0.004105720203369856 185\n",
      "loss 0.014470682479441166 186\n",
      "loss 0.009331491775810719 187\n",
      "loss 0.0033918262924999 188\n",
      "loss 0.003390601137652993 189\n",
      "loss 0.005627920385450125 190\n",
      "loss 0.006116557866334915 191\n",
      "loss 0.0035476740449666977 192\n",
      "loss 0.0050803241319954395 193\n",
      "loss 0.003930704202502966 194\n",
      "loss 0.0010110075818374753 195\n",
      "loss 0.005375927779823542 196\n",
      "loss 0.001143885194323957 197\n",
      "loss 0.003034814028069377 198\n",
      "loss 0.004452297929674387 199\n",
      "Got example 0.001991864526644349 looshx 2.429321527481079 0.001991864526644349 uncertx tensor(0.7018, grad_fn=<MeanBackward0>) 4 200\n",
      "Got example 0.0059544905088841915 looshx 1.9237632751464844 0.0059544905088841915 uncertx tensor(0.7139, grad_fn=<MeanBackward0>) 5 200\n",
      "Got example 0.00530811445787549 looshx 1.6081538200378418 0.00530811445787549 uncertx tensor(0.7019, grad_fn=<MeanBackward0>) 6 200\n",
      "Got example 0.00381956878118217 looshx 1.0060396194458008 0.00381956878118217 uncertx tensor(0.7194, grad_fn=<MeanBackward0>) 7 200\n",
      "Got example 0.002084030769765377 looshx 1.5981849431991577 0.002084030769765377 uncertx tensor(0.7604, grad_fn=<MeanBackward0>) 8 200\n",
      "Got example 0.0017534940270707011 looshx 0.8367025256156921 0.0017534940270707011 uncertx tensor(0.7469, grad_fn=<MeanBackward0>) 9 200\n",
      "Got example 0.0030053078662604094 looshx 2.920753002166748 0.0030053078662604094 uncertx tensor(0.7083, grad_fn=<MeanBackward0>) 10 200\n",
      "Got example 0.012660522013902664 looshx 2.851048469543457 0.012660522013902664 uncertx tensor(0.7026, grad_fn=<MeanBackward0>) 11 200\n",
      "Got example 0.005399174522608519 looshx 1.4547194242477417 0.005399174522608519 uncertx tensor(0.7219, grad_fn=<MeanBackward0>) 12 200\n",
      "Got example 0.002331867115572095 looshx 0.6342769861221313 0.002331867115572095 uncertx tensor(0.7039, grad_fn=<MeanBackward0>) 13 200\n",
      "Got example 0.004589736927300692 looshx 0.5004190802574158 0.004589736927300692 uncertx tensor(0.7499, grad_fn=<MeanBackward0>) 14 200\n",
      "Got example 0.004138834308832884 looshx 1.9868149757385254 0.004138834308832884 uncertx tensor(0.7303, grad_fn=<MeanBackward0>) 15 200\n",
      "Got example 0.004435787908732891 looshx 1.0623866319656372 0.004435787908732891 uncertx tensor(0.7448, grad_fn=<MeanBackward0>) 16 200\n",
      "Got example 0.0024004271253943443 looshx 0.7861085534095764 0.0024004271253943443 uncertx tensor(0.7040, grad_fn=<MeanBackward0>) 17 200\n",
      "Got example 0.002567530609667301 looshx 1.6560947895050049 0.002567530609667301 uncertx tensor(0.7069, grad_fn=<MeanBackward0>) 18 200\n",
      "Got example 0.0017586887115612626 looshx 0.962054431438446 0.0017586887115612626 uncertx tensor(0.7824, grad_fn=<MeanBackward0>) 19 200\n",
      "Got example 0.0017157518304884434 looshx 1.6856049299240112 0.0017157518304884434 uncertx tensor(0.7122, grad_fn=<MeanBackward0>) 20 200\n",
      "Got example 0.001390372752211988 looshx 0.32036975026130676 0.001390372752211988 uncertx tensor(0.7245, grad_fn=<MeanBackward0>) 21 200\n",
      "Got example 0.0036304870154708624 looshx 0.9517666697502136 0.0036304870154708624 uncertx tensor(0.7326, grad_fn=<MeanBackward0>) 22 200\n",
      "Got example 0.004232065286487341 looshx 2.2880046367645264 0.004232065286487341 uncertx tensor(0.7350, grad_fn=<MeanBackward0>) 23 200\n",
      "Got example 0.007126356940716505 looshx 1.6432478427886963 0.007126356940716505 uncertx tensor(0.7120, grad_fn=<MeanBackward0>) 24 200\n",
      "Got example 0.0035353361163288355 looshx 2.5368292331695557 0.0035353361163288355 uncertx tensor(0.7462, grad_fn=<MeanBackward0>) 25 200\n",
      "Got example 0.002263410482555628 looshx 1.9548296928405762 0.002263410482555628 uncertx tensor(0.7100, grad_fn=<MeanBackward0>) 26 200\n",
      "Got example 0.0019940552301704884 looshx 1.9533182382583618 0.0019940552301704884 uncertx tensor(0.7404, grad_fn=<MeanBackward0>) 27 200\n",
      "Got example 0.002552473684772849 looshx 1.3624507188796997 0.002552473684772849 uncertx tensor(0.7056, grad_fn=<MeanBackward0>) 28 200\n",
      "Got example 0.00840391218662262 looshx 2.444694995880127 0.00840391218662262 uncertx tensor(0.7068, grad_fn=<MeanBackward0>) 29 200\n",
      "Got example 0.0012919424334540963 looshx 2.1121914386749268 0.0012919424334540963 uncertx tensor(0.7028, grad_fn=<MeanBackward0>) 30 200\n",
      "Got example 0.002759099006652832 looshx 0.3408009707927704 0.002759099006652832 uncertx tensor(0.8433, grad_fn=<MeanBackward0>) 31 200\n",
      "Got example 0.0014878123765811324 looshx 1.4144748449325562 0.0014878123765811324 uncertx tensor(0.7186, grad_fn=<MeanBackward0>) 32 200\n",
      "Got example 0.002884426387026906 looshx 3.189361810684204 0.002884426387026906 uncertx tensor(0.7121, grad_fn=<MeanBackward0>) 33 200\n",
      "Got example 0.001820972771383822 looshx 0.3051323890686035 0.001820972771383822 uncertx tensor(0.7260, grad_fn=<MeanBackward0>) 34 200\n",
      "Got example 0.001655791886150837 looshx 1.813799500465393 0.001655791886150837 uncertx tensor(0.7115, grad_fn=<MeanBackward0>) 35 200\n",
      "Got example 0.0036977340932935476 looshx 2.3343074321746826 0.0036977340932935476 uncertx tensor(0.7321, grad_fn=<MeanBackward0>) 36 200\n",
      "Got example 0.0024358313530683517 looshx 1.8593679666519165 0.0024358313530683517 uncertx tensor(0.7220, grad_fn=<MeanBackward0>) 37 200\n",
      "Got example 0.0013679092517122626 looshx 1.0587924718856812 0.0013679092517122626 uncertx tensor(0.7163, grad_fn=<MeanBackward0>) 38 200\n",
      "Got example 0.0018565850332379341 looshx 1.626835584640503 0.0018565850332379341 uncertx tensor(0.7081, grad_fn=<MeanBackward0>) 39 200\n",
      "Got example 0.001500318874605 looshx 0.6099711656570435 0.001500318874605 uncertx tensor(0.7108, grad_fn=<MeanBackward0>) 40 200\n",
      "Got example 0.0014061719411984086 looshx 1.5517390966415405 0.0014061719411984086 uncertx tensor(0.7100, grad_fn=<MeanBackward0>) 41 200\n",
      "Got example 0.0016093794256448746 looshx 1.1779241561889648 0.0016093794256448746 uncertx tensor(0.7232, grad_fn=<MeanBackward0>) 42 200\n",
      "Got example 0.0009288115543313324 looshx 1.427146077156067 0.0009288115543313324 uncertx tensor(0.7207, grad_fn=<MeanBackward0>) 43 200\n",
      "Got example 0.0009010252542793751 looshx 0.7368692755699158 0.0009010252542793751 uncertx tensor(0.7562, grad_fn=<MeanBackward0>) 44 200\n",
      "Got example 0.0015948197105899453 looshx 0.35570207238197327 0.0015948197105899453 uncertx tensor(0.7196, grad_fn=<MeanBackward0>) 45 200\n",
      "Got example 0.0013990780571475625 looshx 1.0925025939941406 0.0013990780571475625 uncertx tensor(0.7178, grad_fn=<MeanBackward0>) 46 200\n",
      "Got example 0.015800055116415024 looshx 3.3006598949432373 0.015800055116415024 uncertx tensor(0.7257, grad_fn=<MeanBackward0>) 47 200\n",
      "Got example 0.0014917099615558982 looshx 1.0814188718795776 0.0014917099615558982 uncertx tensor(0.7168, grad_fn=<MeanBackward0>) 48 200\n",
      "Got example 0.0022995758336037397 looshx 1.2863476276397705 0.0022995758336037397 uncertx tensor(0.7052, grad_fn=<MeanBackward0>) 49 200\n",
      "Got example 0.0014055477222427726 looshx 1.6833525896072388 0.0014055477222427726 uncertx tensor(0.7097, grad_fn=<MeanBackward0>) 50 200\n",
      "Got example 0.001516140066087246 looshx 1.059792160987854 0.001516140066087246 uncertx tensor(0.7103, grad_fn=<MeanBackward0>) 51 200\n",
      "Got example 0.003154298523440957 looshx 2.263185739517212 0.003154298523440957 uncertx tensor(0.7131, grad_fn=<MeanBackward0>) 52 200\n",
      "Got example 0.0005956477834843099 looshx 0.936215341091156 0.0005956477834843099 uncertx tensor(0.7069, grad_fn=<MeanBackward0>) 53 200\n",
      "Got example 0.0008399149519391358 looshx 0.5745917558670044 0.0008399149519391358 uncertx tensor(0.7140, grad_fn=<MeanBackward0>) 54 200\n",
      "Got example 0.0006651009316556156 looshx 2.183246374130249 0.0006651009316556156 uncertx tensor(0.7834, grad_fn=<MeanBackward0>) 55 200\n",
      "Got example 0.0015003960579633713 looshx 1.284151315689087 0.0015003960579633713 uncertx tensor(0.7175, grad_fn=<MeanBackward0>) 56 200\n",
      "Got example 0.0026886521372944117 looshx 1.5901166200637817 0.0026886521372944117 uncertx tensor(0.7097, grad_fn=<MeanBackward0>) 57 200\n",
      "Got example 0.0025431818794459105 looshx 2.170225143432617 0.0025431818794459105 uncertx tensor(0.7294, grad_fn=<MeanBackward0>) 58 200\n",
      "Got example 0.017964767292141914 looshx 2.951277256011963 0.017964767292141914 uncertx tensor(0.7410, grad_fn=<MeanBackward0>) 59 200\n",
      "Got example 0.002618409926071763 looshx 1.5475201606750488 0.002618409926071763 uncertx tensor(0.7687, grad_fn=<MeanBackward0>) 60 200\n",
      "Got example 0.0016391839599236846 looshx 2.5586116313934326 0.0016391839599236846 uncertx tensor(0.7134, grad_fn=<MeanBackward0>) 61 200\n",
      "Got example 0.002564984140917659 looshx 1.3557922840118408 0.002564984140917659 uncertx tensor(0.7829, grad_fn=<MeanBackward0>) 62 200\n",
      "Got example 0.0013896996388211846 looshx 1.0618512630462646 0.0013896996388211846 uncertx tensor(0.7072, grad_fn=<MeanBackward0>) 63 200\n",
      "Got example 0.0028399911243468523 looshx 1.859480381011963 0.0028399911243468523 uncertx tensor(0.7778, grad_fn=<MeanBackward0>) 64 200\n",
      "Got example 0.0004427956591825932 looshx 0.5777247548103333 0.0004427956591825932 uncertx tensor(0.7006, grad_fn=<MeanBackward0>) 65 200\n",
      "Got example 0.001737480517476797 looshx 2.194322109222412 0.001737480517476797 uncertx tensor(0.7216, grad_fn=<MeanBackward0>) 66 200\n",
      "Got example 0.0022719481494277716 looshx 2.5756547451019287 0.0022719481494277716 uncertx tensor(0.7280, grad_fn=<MeanBackward0>) 67 200\n",
      "Got example 0.0028918369207531214 looshx 0.5535357594490051 0.0028918369207531214 uncertx tensor(0.7176, grad_fn=<MeanBackward0>) 68 200\n",
      "Got example 0.0009602070203982294 looshx 0.7071285843849182 0.0009602070203982294 uncertx tensor(0.7025, grad_fn=<MeanBackward0>) 69 200\n",
      "Got example 0.002684088656678796 looshx 1.5729939937591553 0.002684088656678796 uncertx tensor(0.7645, grad_fn=<MeanBackward0>) 70 200\n",
      "Got example 0.001336741610430181 looshx 1.883400797843933 0.001336741610430181 uncertx tensor(0.7196, grad_fn=<MeanBackward0>) 71 200\n",
      "Got example 0.001408795709721744 looshx 0.8443068861961365 0.001408795709721744 uncertx tensor(0.7032, grad_fn=<MeanBackward0>) 72 200\n",
      "Got example 0.0038016510661691427 looshx 1.016243577003479 0.0038016510661691427 uncertx tensor(0.7064, grad_fn=<MeanBackward0>) 73 200\n",
      "Got example 0.0006030994700267911 looshx 1.0315537452697754 0.0006030994700267911 uncertx tensor(0.7079, grad_fn=<MeanBackward0>) 74 200\n",
      "Got example 0.002582164481282234 looshx 2.7037625312805176 0.002582164481282234 uncertx tensor(0.7154, grad_fn=<MeanBackward0>) 75 200\n",
      "Got example 0.0011306456290185452 looshx 0.4555204212665558 0.0011306456290185452 uncertx tensor(0.8047, grad_fn=<MeanBackward0>) 76 200\n",
      "Got example 0.0009287253487855196 looshx 0.7694876194000244 0.0009287253487855196 uncertx tensor(0.7725, grad_fn=<MeanBackward0>) 77 200\n",
      "Got example 0.0007726020994596183 looshx 1.1876232624053955 0.0007726020994596183 uncertx tensor(0.7279, grad_fn=<MeanBackward0>) 78 200\n",
      "Got example 0.002034959848970175 looshx 0.9547507762908936 0.002034959848970175 uncertx tensor(0.8002, grad_fn=<MeanBackward0>) 79 200\n",
      "Got example 0.002262985333800316 looshx 2.319500684738159 0.002262985333800316 uncertx tensor(0.7348, grad_fn=<MeanBackward0>) 80 200\n",
      "Got example 0.0018471608636900783 looshx 0.35832250118255615 0.0018471608636900783 uncertx tensor(0.7058, grad_fn=<MeanBackward0>) 81 200\n",
      "Got example 0.002216057851910591 looshx 2.4180679321289062 0.002216057851910591 uncertx tensor(0.7072, grad_fn=<MeanBackward0>) 82 200\n",
      "Got example 0.0014977296814322472 looshx 2.548887014389038 0.0014977296814322472 uncertx tensor(0.7100, grad_fn=<MeanBackward0>) 83 200\n",
      "Got example 0.0007093206513673067 looshx 0.7722000479698181 0.0007093206513673067 uncertx tensor(0.7050, grad_fn=<MeanBackward0>) 84 200\n",
      "Got example 0.0018204264342784882 looshx 1.6725361347198486 0.0018204264342784882 uncertx tensor(0.7328, grad_fn=<MeanBackward0>) 85 200\n",
      "Got example 0.0012479409342631698 looshx 1.5187920331954956 0.0012479409342631698 uncertx tensor(0.7731, grad_fn=<MeanBackward0>) 86 200\n",
      "Got example 0.00037847310886718333 looshx 0.5620614290237427 0.00037847310886718333 uncertx tensor(0.7003, grad_fn=<MeanBackward0>) 87 200\n",
      "Got example 0.000723190198186785 looshx 0.5830607414245605 0.000723190198186785 uncertx tensor(0.7120, grad_fn=<MeanBackward0>) 88 200\n",
      "Got example 0.0004426447849255055 looshx 1.2353804111480713 0.0004426447849255055 uncertx tensor(0.7172, grad_fn=<MeanBackward0>) 89 200\n",
      "Got example 0.0006827029283158481 looshx 1.8682949542999268 0.0006827029283158481 uncertx tensor(0.7500, grad_fn=<MeanBackward0>) 90 200\n",
      "Got example 0.0016752127557992935 looshx 1.6449146270751953 0.0016752127557992935 uncertx tensor(0.7372, grad_fn=<MeanBackward0>) 91 200\n",
      "Got example 0.0023379314225167036 looshx 0.896479070186615 0.0023379314225167036 uncertx tensor(0.7063, grad_fn=<MeanBackward0>) 92 200\n",
      "Got example 0.001811894471757114 looshx 1.2225258350372314 0.001811894471757114 uncertx tensor(0.7095, grad_fn=<MeanBackward0>) 93 200\n",
      "Got example 0.0008666375069878995 looshx 2.301287889480591 0.0008666375069878995 uncertx tensor(0.7793, grad_fn=<MeanBackward0>) 94 200\n",
      "Got example 0.002175601664930582 looshx 1.0411540269851685 0.002175601664930582 uncertx tensor(0.7193, grad_fn=<MeanBackward0>) 95 200\n",
      "Got example 0.0015526356874033809 looshx 1.123461365699768 0.0015526356874033809 uncertx tensor(0.7066, grad_fn=<MeanBackward0>) 96 200\n",
      "Got example 0.0019256416708230972 looshx 3.3267128467559814 0.0019256416708230972 uncertx tensor(0.7999, grad_fn=<MeanBackward0>) 97 200\n",
      "Got example 0.0010674159275367856 looshx 0.865151584148407 0.0010674159275367856 uncertx tensor(0.7383, grad_fn=<MeanBackward0>) 98 200\n",
      "Got example 0.003932420630007982 looshx 1.8230382204055786 0.003932420630007982 uncertx tensor(0.7385, grad_fn=<MeanBackward0>) 99 200\n",
      "Got example 0.0003944210475310683 looshx 0.3516736626625061 0.0003944210475310683 uncertx tensor(0.7624, grad_fn=<MeanBackward0>) 100 200\n",
      "Got example 0.0018795301439240575 looshx 0.7163580656051636 0.0018795301439240575 uncertx tensor(0.7504, grad_fn=<MeanBackward0>) 101 200\n",
      "Got example 0.0006621028878726065 looshx 0.7730405926704407 0.0006621028878726065 uncertx tensor(0.7008, grad_fn=<MeanBackward0>) 102 200\n",
      "Got example 0.0019338902784511447 looshx 1.8476582765579224 0.0019338902784511447 uncertx tensor(0.7080, grad_fn=<MeanBackward0>) 103 200\n",
      "Got example 0.0020566601306200027 looshx 1.6012053489685059 0.0020566601306200027 uncertx tensor(0.7246, grad_fn=<MeanBackward0>) 104 200\n",
      "Got example 0.00020195619435980916 looshx 0.9866644740104675 0.00020195619435980916 uncertx tensor(0.7035, grad_fn=<MeanBackward0>) 105 200\n",
      "Got example 0.003525438020005822 looshx 1.6187615394592285 0.003525438020005822 uncertx tensor(0.7031, grad_fn=<MeanBackward0>) 106 200\n",
      "Got example 0.0021419075783342123 looshx 0.5641807913780212 0.0021419075783342123 uncertx tensor(0.7256, grad_fn=<MeanBackward0>) 107 200\n",
      "Got example 0.0009032900561578572 looshx 1.627543330192566 0.0009032900561578572 uncertx tensor(0.7227, grad_fn=<MeanBackward0>) 108 200\n",
      "Got example 0.0020758570171892643 looshx 1.4077060222625732 0.0020758570171892643 uncertx tensor(0.7013, grad_fn=<MeanBackward0>) 109 200\n",
      "Got example 0.005714261904358864 looshx 2.481928825378418 0.005714261904358864 uncertx tensor(0.7086, grad_fn=<MeanBackward0>) 110 200\n",
      "Got example 0.004564316011965275 looshx 1.9246413707733154 0.004564316011965275 uncertx tensor(0.7012, grad_fn=<MeanBackward0>) 111 200\n",
      "Got example 0.00153907120693475 looshx 1.0251834392547607 0.00153907120693475 uncertx tensor(0.7238, grad_fn=<MeanBackward0>) 112 200\n",
      "Got example 0.003926645498722792 looshx 0.7796940803527832 0.003926645498722792 uncertx tensor(0.7032, grad_fn=<MeanBackward0>) 113 200\n",
      "Got example 0.0019632340408861637 looshx 0.9875442385673523 0.0019632340408861637 uncertx tensor(0.7194, grad_fn=<MeanBackward0>) 114 200\n",
      "Got example 0.0008604335598647594 looshx 0.6747971177101135 0.0008604335598647594 uncertx tensor(0.7209, grad_fn=<MeanBackward0>) 115 200\n",
      "Got example 0.0023917744401842356 looshx 2.7089884281158447 0.0023917744401842356 uncertx tensor(0.7153, grad_fn=<MeanBackward0>) 116 200\n",
      "Got example 0.004534720443189144 looshx 1.9441416263580322 0.004534720443189144 uncertx tensor(0.7008, grad_fn=<MeanBackward0>) 117 200\n",
      "Got example 0.0034276230726391077 looshx 2.5435471534729004 0.0034276230726391077 uncertx tensor(0.7279, grad_fn=<MeanBackward0>) 118 200\n",
      "Got example 0.0010986871784552932 looshx 1.827376365661621 0.0010986871784552932 uncertx tensor(0.7259, grad_fn=<MeanBackward0>) 119 200\n",
      "Got example 0.0019120402866974473 looshx 1.587174654006958 0.0019120402866974473 uncertx tensor(0.7074, grad_fn=<MeanBackward0>) 120 200\n",
      "Got example 0.003897310933098197 looshx 1.3777825832366943 0.003897310933098197 uncertx tensor(0.7365, grad_fn=<MeanBackward0>) 121 200\n",
      "Got example 0.0020452081225812435 looshx 2.659090280532837 0.0020452081225812435 uncertx tensor(0.7220, grad_fn=<MeanBackward0>) 122 200\n",
      "Got example 0.0006313054473139346 looshx 1.30402672290802 0.0006313054473139346 uncertx tensor(0.7349, grad_fn=<MeanBackward0>) 123 200\n",
      "Got example 0.0012555500725284219 looshx 0.906475841999054 0.0012555500725284219 uncertx tensor(0.7061, grad_fn=<MeanBackward0>) 124 200\n",
      "Got example 0.009791300632059574 looshx 2.3815858364105225 0.009791300632059574 uncertx tensor(0.7094, grad_fn=<MeanBackward0>) 125 200\n",
      "Got example 0.006937942001968622 looshx 1.7351733446121216 0.006937942001968622 uncertx tensor(0.7582, grad_fn=<MeanBackward0>) 126 200\n",
      "Got example 0.0008725132211111486 looshx 2.1990928649902344 0.0008725132211111486 uncertx tensor(0.7843, grad_fn=<MeanBackward0>) 127 200\n",
      "Got example 0.001252468558959663 looshx 2.2193665504455566 0.001252468558959663 uncertx tensor(0.7064, grad_fn=<MeanBackward0>) 128 200\n",
      "Got example 0.0003644858079496771 looshx 0.6883112788200378 0.0003644858079496771 uncertx tensor(0.7095, grad_fn=<MeanBackward0>) 129 200\n",
      "Got example 0.000365635147318244 looshx 1.2081127166748047 0.000365635147318244 uncertx tensor(0.7020, grad_fn=<MeanBackward0>) 130 200\n",
      "Got example 0.0024761033710092306 looshx 2.9747796058654785 0.0024761033710092306 uncertx tensor(0.7085, grad_fn=<MeanBackward0>) 131 200\n",
      "Got example 0.0003054705448448658 looshx 2.009323835372925 0.0003054705448448658 uncertx tensor(0.7173, grad_fn=<MeanBackward0>) 132 200\n",
      "Got example 0.000725473219063133 looshx 1.1013957262039185 0.000725473219063133 uncertx tensor(0.7003, grad_fn=<MeanBackward0>) 133 200\n",
      "Got example 0.001477301586419344 looshx 0.9793602824211121 0.001477301586419344 uncertx tensor(0.7064, grad_fn=<MeanBackward0>) 134 200\n",
      "Got example 0.0007744001341052353 looshx 0.37564554810523987 0.0007744001341052353 uncertx tensor(0.7104, grad_fn=<MeanBackward0>) 135 200\n",
      "Got example 0.0004984752740710974 looshx 0.593722403049469 0.0004984752740710974 uncertx tensor(0.7933, grad_fn=<MeanBackward0>) 136 200\n",
      "Got example 0.0003178700862918049 looshx 0.7395510077476501 0.0003178700862918049 uncertx tensor(0.7161, grad_fn=<MeanBackward0>) 137 200\n",
      "Got example 0.0009644945967011154 looshx 0.8703190088272095 0.0009644945967011154 uncertx tensor(0.7131, grad_fn=<MeanBackward0>) 138 200\n",
      "Got example 0.0014866986311972141 looshx 1.8604507446289062 0.0014866986311972141 uncertx tensor(0.7662, grad_fn=<MeanBackward0>) 139 200\n",
      "Got example 0.0003387796168681234 looshx 0.5753562450408936 0.0003387796168681234 uncertx tensor(0.7328, grad_fn=<MeanBackward0>) 140 200\n",
      "Got example 0.0023458197247236967 looshx 1.2154490947723389 0.0023458197247236967 uncertx tensor(0.7069, grad_fn=<MeanBackward0>) 141 200\n",
      "Got example 0.00034344886080361903 looshx 0.8125112652778625 0.00034344886080361903 uncertx tensor(0.7306, grad_fn=<MeanBackward0>) 142 200\n",
      "Got example 0.00353806815110147 looshx 2.5827770233154297 0.00353806815110147 uncertx tensor(0.7009, grad_fn=<MeanBackward0>) 143 200\n",
      "Got example 0.0004256997490301728 looshx 0.5121744871139526 0.0004256997490301728 uncertx tensor(0.7057, grad_fn=<MeanBackward0>) 144 200\n",
      "Got example 0.0019053473370149732 looshx 0.8449234962463379 0.0019053473370149732 uncertx tensor(0.7484, grad_fn=<MeanBackward0>) 145 200\n",
      "Got example 0.0015372927300632 looshx 1.0122010707855225 0.0015372927300632 uncertx tensor(0.7047, grad_fn=<MeanBackward0>) 146 200\n",
      "Got example 0.001085541327483952 looshx 0.14725859463214874 0.001085541327483952 uncertx tensor(0.7200, grad_fn=<MeanBackward0>) 147 200\n",
      "Got example 0.001011199434287846 looshx 1.902613878250122 0.001011199434287846 uncertx tensor(0.7227, grad_fn=<MeanBackward0>) 148 200\n",
      "Got example 0.0009606407838873565 looshx 1.4592169523239136 0.0009606407838873565 uncertx tensor(0.7055, grad_fn=<MeanBackward0>) 149 200\n",
      "Got example 0.001281940029002726 looshx 1.1852415800094604 0.001281940029002726 uncertx tensor(0.7200, grad_fn=<MeanBackward0>) 150 200\n",
      "Got example 0.00039519392885267735 looshx 1.129791021347046 0.00039519392885267735 uncertx tensor(0.7422, grad_fn=<MeanBackward0>) 151 200\n",
      "Got example 0.0010392562253400683 looshx 1.2787134647369385 0.0010392562253400683 uncertx tensor(0.7056, grad_fn=<MeanBackward0>) 152 200\n",
      "Got example 0.0006503494805656374 looshx 1.5825862884521484 0.0006503494805656374 uncertx tensor(0.7394, grad_fn=<MeanBackward0>) 153 200\n",
      "Got example 0.0008542138966731727 looshx 0.5011778473854065 0.0008542138966731727 uncertx tensor(0.7113, grad_fn=<MeanBackward0>) 154 200\n",
      "Got example 0.0024224764201790094 looshx 1.8862186670303345 0.0024224764201790094 uncertx tensor(0.7035, grad_fn=<MeanBackward0>) 155 200\n",
      "Got example 0.003580763703212142 looshx 3.2138307094573975 0.003580763703212142 uncertx tensor(0.7032, grad_fn=<MeanBackward0>) 156 200\n",
      "Got example 0.0009333871421404183 looshx 2.131863594055176 0.0009333871421404183 uncertx tensor(0.8089, grad_fn=<MeanBackward0>) 157 200\n",
      "Got example 0.0008145503816194832 looshx 0.9922277927398682 0.0008145503816194832 uncertx tensor(0.7068, grad_fn=<MeanBackward0>) 158 200\n",
      "Got example 0.0011734248837456107 looshx 1.0065969228744507 0.0011734248837456107 uncertx tensor(0.7420, grad_fn=<MeanBackward0>) 159 200\n",
      "Got example 0.001561939250677824 looshx 1.040703535079956 0.001561939250677824 uncertx tensor(0.7328, grad_fn=<MeanBackward0>) 160 200\n",
      "Got example 0.0008977128309197724 looshx 3.37664794921875 0.0008977128309197724 uncertx tensor(0.7654, grad_fn=<MeanBackward0>) 161 200\n",
      "Got example 0.0004929429269395769 looshx 0.924557089805603 0.0004929429269395769 uncertx tensor(0.7280, grad_fn=<MeanBackward0>) 162 200\n",
      "Got example 0.0028161208610981703 looshx 1.2971657514572144 0.0028161208610981703 uncertx tensor(0.7106, grad_fn=<MeanBackward0>) 163 200\n",
      "Got example 0.0011147180339321494 looshx 0.6330609917640686 0.0011147180339321494 uncertx tensor(0.7231, grad_fn=<MeanBackward0>) 164 200\n",
      "Got example 0.0005520652630366385 looshx 1.1901825666427612 0.0005520652630366385 uncertx tensor(0.7014, grad_fn=<MeanBackward0>) 165 200\n",
      "Got example 0.0004520240181591362 looshx 0.4412013292312622 0.0004520240181591362 uncertx tensor(0.7679, grad_fn=<MeanBackward0>) 166 200\n",
      "Got example 0.0011947584571316838 looshx 2.148369073867798 0.0011947584571316838 uncertx tensor(0.7072, grad_fn=<MeanBackward0>) 167 200\n",
      "Got example 0.0003853705420624465 looshx 1.4017620086669922 0.0003853705420624465 uncertx tensor(0.7245, grad_fn=<MeanBackward0>) 168 200\n",
      "Got example 0.0006069883820600808 looshx 1.9669398069381714 0.0006069883820600808 uncertx tensor(0.7235, grad_fn=<MeanBackward0>) 169 200\n",
      "Got example 0.0004365643544588238 looshx 0.8214951753616333 0.0004365643544588238 uncertx tensor(0.7739, grad_fn=<MeanBackward0>) 170 200\n",
      "Got example 0.0011710881954059005 looshx 1.0642248392105103 0.0011710881954059005 uncertx tensor(0.7058, grad_fn=<MeanBackward0>) 171 200\n",
      "Got example 0.0012194182490929961 looshx 0.5092772245407104 0.0012194182490929961 uncertx tensor(0.7281, grad_fn=<MeanBackward0>) 172 200\n",
      "Got example 0.0035548501182347536 looshx 2.622631072998047 0.0035548501182347536 uncertx tensor(0.7085, grad_fn=<MeanBackward0>) 173 200\n",
      "Got example 0.0006197844631969929 looshx 1.3606423139572144 0.0006197844631969929 uncertx tensor(0.7066, grad_fn=<MeanBackward0>) 174 200\n",
      "Got example 0.0015578578459098935 looshx 2.3834593296051025 0.0015578578459098935 uncertx tensor(0.7015, grad_fn=<MeanBackward0>) 175 200\n",
      "Got example 0.00042055980884470046 looshx 1.0715899467468262 0.00042055980884470046 uncertx tensor(0.7012, grad_fn=<MeanBackward0>) 176 200\n",
      "Got example 0.0029748089145869017 looshx 2.5147414207458496 0.0029748089145869017 uncertx tensor(0.7220, grad_fn=<MeanBackward0>) 177 200\n",
      "Got example 0.004019212443381548 looshx 1.6349002122879028 0.004019212443381548 uncertx tensor(0.7364, grad_fn=<MeanBackward0>) 178 200\n",
      "Got example 0.0010254320222884417 looshx 1.7706632614135742 0.0010254320222884417 uncertx tensor(0.7137, grad_fn=<MeanBackward0>) 179 200\n",
      "Got example 0.00045153024257160723 looshx 0.9071578979492188 0.00045153024257160723 uncertx tensor(0.7051, grad_fn=<MeanBackward0>) 180 200\n",
      "Got example 0.0026826998218894005 looshx 1.7782299518585205 0.0026826998218894005 uncertx tensor(0.7433, grad_fn=<MeanBackward0>) 181 200\n",
      "Got example 0.0017863702960312366 looshx 2.5329065322875977 0.0017863702960312366 uncertx tensor(0.7363, grad_fn=<MeanBackward0>) 182 200\n",
      "Got example 0.004609523806720972 looshx 1.743510365486145 0.004609523806720972 uncertx tensor(0.7373, grad_fn=<MeanBackward0>) 183 200\n",
      "Got example 0.00045280123595148325 looshx 0.7956883907318115 0.00045280123595148325 uncertx tensor(0.7268, grad_fn=<MeanBackward0>) 184 200\n",
      "Got example 0.0010947732953354716 looshx 1.0521577596664429 0.0010947732953354716 uncertx tensor(0.7535, grad_fn=<MeanBackward0>) 185 200\n",
      "Got example 0.0005018236697651446 looshx 1.8024399280548096 0.0005018236697651446 uncertx tensor(0.7055, grad_fn=<MeanBackward0>) 186 200\n",
      "Got example 0.000684439844917506 looshx 1.8231390714645386 0.000684439844917506 uncertx tensor(0.7428, grad_fn=<MeanBackward0>) 187 200\n",
      "Got example 0.0004995429189875722 looshx 1.2802903652191162 0.0004995429189875722 uncertx tensor(0.7138, grad_fn=<MeanBackward0>) 188 200\n",
      "Got example 0.000751596235204488 looshx 0.06471754610538483 0.000751596235204488 uncertx tensor(0.7806, grad_fn=<MeanBackward0>) 189 200\n",
      "Got example 0.0006614716839976609 looshx 0.9937524795532227 0.0006614716839976609 uncertx tensor(0.7158, grad_fn=<MeanBackward0>) 190 200\n",
      "Got example 0.0005238332087174058 looshx 0.9568068385124207 0.0005238332087174058 uncertx tensor(0.7626, grad_fn=<MeanBackward0>) 191 200\n",
      "Got example 0.0008982556755654514 looshx 1.404649019241333 0.0008982556755654514 uncertx tensor(0.7109, grad_fn=<MeanBackward0>) 192 200\n",
      "Got example 0.0009433392551727593 looshx 0.4394509792327881 0.0009433392551727593 uncertx tensor(0.7158, grad_fn=<MeanBackward0>) 193 200\n",
      "Got example 0.0005654108244925737 looshx 0.9110532402992249 0.0005654108244925737 uncertx tensor(0.7064, grad_fn=<MeanBackward0>) 194 200\n",
      "Got example 0.0014034537598490715 looshx 0.9442693591117859 0.0014034537598490715 uncertx tensor(0.7015, grad_fn=<MeanBackward0>) 195 200\n",
      "Got example 0.0022607932332903147 looshx 1.542160987854004 0.0022607932332903147 uncertx tensor(0.7062, grad_fn=<MeanBackward0>) 196 200\n",
      "Got example 0.00023378075275104493 looshx 0.635344386100769 0.00023378075275104493 uncertx tensor(0.7168, grad_fn=<MeanBackward0>) 197 200\n",
      "Got example 0.0049203503876924515 looshx 1.780113935470581 0.0049203503876924515 uncertx tensor(0.7278, grad_fn=<MeanBackward0>) 198 200\n",
      "Got example 0.000375447329133749 looshx 0.30523648858070374 0.000375447329133749 uncertx tensor(0.7044, grad_fn=<MeanBackward0>) 199 200\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_nodes = 6\n",
    "feature_dim = 1\n",
    "hidden_dim =10\n",
    "output_dim = 1\n",
    "cutoff = 1.5\n",
    "device = 'cpu'\n",
    "n_samples=10\n",
    "# Example: Prune weights, freeze pruned parameters, and train on uncertain data\n",
    "# Example model instantiation and optimizer setup\n",
    "model = EGNN(\n",
    "    in_node_nf=feature_dim,\n",
    "    hidden_nf=hidden_dim,\n",
    "    out_node_nf=output_dim,\n",
    "    n_layers=3\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "k=0\n",
    "j=0\n",
    "model.train()  # Set model to training mode\n",
    "loss_fn = nn.MSELoss()\n",
    "# train a model selecting these elements of the training set that are below an uncertainty (h_uncertainty > 0.0002) & (x_uncertainty > 0.6)\n",
    "while j<200:\n",
    "    train_data = generate_data(num_nodes, feature_dim, cutoff)\n",
    "    # Uncertainty sampling for training\n",
    "    node_features, node_coords, edge_indices = train_data  # Assuming train_data is in this format\n",
    "    target_coords=update_coordinates(node_coords, node_features)\n",
    "    target_h=torch.norm(node_features)\n",
    "    # Get mean predictions and uncertainties\n",
    "    (h_mean, h_uncertainty), (x_mean, x_uncertainty) = model.forward_with_uncertainty(node_features, node_coords, edge_indices, n_samples)\n",
    "    # Use uncertainty to filter out high-uncertainty predictions for training\n",
    "    mask = (h_uncertainty > 0.0002) & (x_uncertainty > 0.7)\n",
    "    # mask = (x_uncertainty > 0.5)\n",
    "    # print(\"uncer hx\",h_uncertainty.item(),x_uncertainty.item())\n",
    "    if (mask and j<200):\n",
    "        # Perform pruning\n",
    "        prune_weights(model)\n",
    "        # Freeze pruned parameters\n",
    "        freeze_parameters(model)\n",
    "        # Apply the mask to h_mean and x_mean (you need to ensure that the mask applies element-wise)\n",
    "        optimizer.zero_grad()\n",
    "        loss_coord = loss_fn(target_coords,x_mean)  # Compute loss on uncertain data\n",
    "        loss_h= loss_fn(target_h,torch.norm(h_mean))\n",
    "        loss= loss_coord+0.0*loss_h\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Got example\",loss.item(),\"looshx\", loss_h.item(),loss_coord.item(),\"uncertx\",x_uncertainty,j,k)\n",
    "        j+=1\n",
    "    if ((k<200) and (mask==False)):\n",
    "        optimizer.zero_grad()\n",
    "        loss_coord = loss_fn(target_coords,x_mean)  # Compute loss on uncertain data\n",
    "        loss_h= loss_fn(target_h,torch.norm(h_mean))\n",
    "        loss= loss_coord+0.00*loss_h\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"loss\",loss.item(),k)\n",
    "        k+=1\n",
    "print(k+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a13ce8-3fd8-44a8-a597-29db1fe62222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.004231675527989864 400\n"
     ]
    }
   ],
   "source": [
    "#train a baseline model 100 epochs\n",
    "# Hyperparameters\n",
    "num_nodes = 6\n",
    "feature_dim = 1\n",
    "hidden_dim =10\n",
    "output_dim = 1\n",
    "cutoff = 1.5\n",
    "device = 'cpu'\n",
    "n_samples=10\n",
    "# Example: Prune weights, freeze pruned parameters, and train on uncertain data\n",
    "# Example model instantiation and optimizer setup\n",
    "egnn2 = EGNN(\n",
    "    in_node_nf=feature_dim,\n",
    "    hidden_nf=hidden_dim,\n",
    "    out_node_nf=output_dim,\n",
    "    n_layers=3\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(egnn2.parameters(), lr=1e-3)\n",
    "k=0\n",
    "j=0\n",
    "egnn2.train()  # Set model to training mode\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "while j<400:\n",
    "    train_data = generate_data(num_nodes, feature_dim, cutoff)\n",
    "    # Perform pruning\n",
    "    # prune_weights(egnn2)\n",
    "    # # Freeze pruned parameters\n",
    "    # freeze_parameters(egnn2)\n",
    "    # Uncertainty sampling for training\n",
    "    node_features, node_coords, edge_indices = train_data  # Assuming train_data is in this format\n",
    "    target_coords=update_coordinates(node_coords, node_features)\n",
    "    target_h=torch.norm(node_features)\n",
    "    # Get mean predictions and uncertainties\n",
    "    (h_mean, h_uncertainty), (x_mean, x_uncertainty) = egnn2.forward_with_uncertainty(node_features, node_coords, edge_indices, n_samples)\n",
    "    # Use uncertainty to filter out high-uncertainty predictions for training\n",
    "    mask = (h_uncertainty < 0.03) & (x_uncertainty < 0.4)\n",
    "    mask = (x_uncertainty < 0.6)\n",
    "    mask = True\n",
    "    # print(\"uncer hx\",h_uncertainty.item(),x_uncertainty.item())\n",
    "    if mask:\n",
    "        # Apply the mask to h_mean and x_mean (you need to ensure that the mask applies element-wise)\n",
    "        optimizer.zero_grad()\n",
    "        loss_coord = loss_fn(target_coords,x_mean)  # Compute loss on uncertain data\n",
    "        loss_h= loss_fn(target_h,torch.norm(h_mean))\n",
    "        loss= loss_coord+0.0*loss_h\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"Got example\",loss.item(),\"looshx\", loss_h.item(),loss_coord.item(),\"uncertx\",x_uncertainty,j)\n",
    "        j+=1\n",
    "        k+=1\n",
    "    # if k<100:\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss_coord = loss_fn(target_coords,x_mean)  # Compute loss on uncertain data\n",
    "    #     loss_h= loss_fn(target_h,torch.norm(h_mean))\n",
    "    #     loss= loss_coord+0.00*loss_h\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "print(\"loss\",loss.item(),k)\n",
    "    #     k+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9fac17-d78d-4120-a0a7-38a9823ffa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Pruned - Error baseline -0.0003321294207125902 0.6098178029060364 0.6140052080154419\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0003321294207125902\n"
     ]
    }
   ],
   "source": [
    "#now plot the results the red coordinatres yellow dots coordinates baseline and red dots our model\n",
    "egnn2.eval()\n",
    "model.eval()\n",
    "n_samples=10\n",
    "num_nodes =7\n",
    "loss_fn = nn.MSELoss()\n",
    "a=0\n",
    "for _ in range(10):\n",
    "    train_data = generate_data(num_nodes, feature_dim, cutoff)\n",
    "    node_features, node_coords, edge_indices = train_data  # Assuming train_data is in this format\n",
    "    target_coords=update_coordinates(node_coords, node_features)\n",
    "    target_h=torch.norm(node_features)\n",
    "    (h_mean, h_uncertainty), (x_mean, x_uncertainty) = model.forward_with_uncertainty(node_features, node_coords,edge_indices)\n",
    "    if x_uncertainty.item()>0.6:\n",
    "        \n",
    "        loss_coordm = loss_fn(target_coords,x_mean)\n",
    "        (h_mean, h_uncertainty), (x_mean2, x_uncertainty2) = egnn2.forward_with_uncertainty(node_features, node_coords, edge_indices)\n",
    "        loss_coordegnn2 = loss_fn(target_coords,x_mean2)\n",
    "        a+=loss_coordm.item()-loss_coordegnn2.item()\n",
    "        # print((x_uncertainty-x_uncertainty2).item())\n",
    "        print(\"Error Pruned - Error baseline\",loss_coordm.item()-loss_coordegnn2.item(),x_uncertainty2.item(),x_uncertainty.item())\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # plt.scatter(node_coords[:, 0], node_coords[:, 1], color='blue', label='Original Coordinates')\n",
    "        plt.scatter(x_mean.detach().numpy()[:, 0], x_mean.detach().numpy()[:, 1], color='red',marker='x',label='Pruned model')\n",
    "        plt.scatter(x_mean2.detach().numpy()[:, 0], x_mean2.detach().numpy()[:, 1], color='yellow',label='Baseline model')\n",
    "        plt.scatter(target_coords[:, 0], target_coords[:, 1], color='brown',facecolors='none',label='Target Coordinates')\n",
    "        # plt.scatter(dif[:, 0], dif[:, 1], color='yellow', label='diff')\n",
    "        plt.legend()\n",
    "        plt.title(\"Baseline vs Pruned model\")\n",
    "        plt.show()\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601aaff3-2ab8-46d8-bac3-39dd67552dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c440e8-e553-4145-83ae-e994787274bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
